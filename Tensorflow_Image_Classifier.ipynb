{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_0ZOlOW7jom",
        "outputId": "03aaf5eb-74bb-4619-e6d4-be728bcf2e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yk6aJdQs8asv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"x train shape: \",x_train.shape)\n",
        "print(\"y test shape: \",x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93xsWacW_vN9",
        "outputId": "9e9acb01-12e4-44d7-f1b7-04b550cfd417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "x train shape:  (60000, 28, 28)\n",
            "y test shape:  (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "DG4cCSI3Dlgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNGvAeyVjTbL",
        "outputId": "d8f6a2e2-21f7-4301-f84b-c64efaefaa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  tf.keras.models.Sequential(): Creates a simple, linear stack of layers for the neural network.\n",
        "*   tf.keras.layers.Flatten(input_shape=(28, 28)): This layer flattens the 28x28 input images into a 1D array of 784 pixels (28 * 28 = 784), which is required for fully connected layers.\n",
        "*  tf.keras.layers.Dense(128, activation='relu'): A fully connected (dense) layer with 128 neurons and the ReLU activation function. The ReLU function introduces non-linearity, enabling the network to learn complex patterns.\n",
        "* tf.keras.layers.Dropout(0.2): A dropout layer that randomly \"drops\" (sets to zero) 20% of the neurons during training to prevent overfitting. By doing so, it forces the network to learn more robust features rather than relying on specific neurons.\n",
        "* tf.keras.layers.Dense(10): The output layer has 10 neurons corresponding to the 10 possible classes (digits 0-9). It uses a linear activation function by default.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yl-JzjjRBctf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hbf-JMdHhnQ",
        "outputId": "f1197295-3637-4d77-8bf3-fe37c7760d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
            "  0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
            "  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
            "  0.99215686 0.35294118 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.54509804\n",
            "  0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.04313725\n",
            "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
            "  0.09803922 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            "  0.58823529 0.10588235 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
            "  0.99215686 0.73333333 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.97647059\n",
            "  0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
            "  0.98039216 0.71372549 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.09411765 0.44705882\n",
            "  0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
            "  0.30588235 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.21568627 0.6745098\n",
            "  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
            "  0.52156863 0.04313725 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.53333333 0.99215686\n",
            "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(x_train[:1])\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBU9yqCWr7kh",
        "outputId": "4d58774c-fa77-464d-e71c-3a94adb4ba4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[-13.078039 ,  -5.698321 ,  -2.2054358,  10.04821  , -24.430523 ,\n",
              "         11.505349 , -18.72177  ,  -6.1426544, -12.91408  ,  -6.521214 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bczDs7vMVd0",
        "outputId": "13f87447-91c9-47bb-a244-a9f188c4ec70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-13.078039 ,  -5.698321 ,  -2.2054358,  10.04821  , -24.430523 ,\n",
              "         11.505349 , -18.72177  ,  -6.1426544, -12.91408  ,  -6.521214 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model(x_train[:1]): You pass a batch of input data (x_train[:1], which is the first image in the dataset) to the model.\n",
        "\n",
        "The model computes a prediction for this image. This prediction is not a direct label (like 0, 1, 2), but a set of logits. These logits are raw outputs from the model (before they are converted to probabilities), and each logit corresponds to a \"score\" for each class (digits 0-9 in this case).\n",
        "\n",
        "So, predictions is a tensor with shape (1, 10) (1 image, 10 possible classes). The values represent the model’s raw predictions (logits) for each class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aS9o2dVaNPEx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecXJNVmONMyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz72EjnL1D9S",
        "outputId": "046625a4-01db-43f8-d174-e1fd16e0cf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05533126, 0.0758725 , 0.09113963, 0.0929936 , 0.04491719,\n",
              "        0.13898942, 0.14895822, 0.15153258, 0.13664864, 0.06361692]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Softmax function converts the logits into probabilities, where the sum of all 10 output values will be 1, and each value represents the probability of the image belonging to a particular class (digit 0-9)."
      ],
      "metadata": {
        "id": "uUApnXw_DvF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "5svy6ysy572z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) defines the loss function used to calculate how far the model's predictions are from the true labels.\n",
        "\n",
        "This function is suitable for multi-class classification problems where the labels are integers (not one-hot encoded).\n",
        "\n",
        "from_logits=True specifies that the model outputs raw logits (not probabilities), so the loss function will internally apply Softmax to the logits.\n",
        "\n"
      ],
      "metadata": {
        "id": "1AqnkfS5D3ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vec6P87P5-H2",
        "outputId": "abf2546c-256b-4988-c0a2-bdf12d103d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(1.9733574)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0BZygqYu6Fcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tFPj9enEcCh",
        "outputId": "b75d6e55-4295-4b6d-a346-b595fa5ab011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8665 - loss: 0.4659\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1448\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9672 - loss: 0.1058\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0835\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9772 - loss: 0.0720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79652944eb50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eAuR1-gE9Kq",
        "outputId": "1585908c-9971-45ff-c387-84ebdf319bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - 3ms/step - accuracy: 0.9786 - loss: 0.0695\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06951350718736649, 0.978600025177002]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "metadata": {
        "id": "KWBUKrKRFqko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probability_model(x_test[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MBgRIQNFwzZ",
        "outputId": "379f41e4-a7db-4dcf-ef70-84d8b4d7447c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[1.8560739e-08, 2.0493431e-09, 2.8229200e-05, 5.2320538e-04,\n",
              "        2.1597375e-09, 2.2805173e-07, 7.9336180e-13, 9.9937290e-01,\n",
              "        5.6077993e-06, 6.9755552e-05],\n",
              "       [2.5326350e-09, 2.8616787e-06, 9.9999499e-01, 2.0348616e-06,\n",
              "        4.5062126e-16, 7.5140001e-08, 8.5363722e-10, 7.0283893e-15,\n",
              "        8.2256388e-09, 1.6509733e-14],\n",
              "       [3.9340126e-08, 9.9868363e-01, 4.3213255e-05, 1.0642373e-06,\n",
              "        3.2629436e-05, 1.5742535e-05, 7.6435244e-06, 1.1664330e-03,\n",
              "        4.8532944e-05, 1.0099951e-06],\n",
              "       [9.9873430e-01, 1.2993516e-08, 1.1583973e-03, 4.3123491e-06,\n",
              "        3.2072234e-07, 1.8207531e-05, 3.4055531e-06, 2.2155032e-06,\n",
              "        8.7198822e-07, 7.7932971e-05],\n",
              "       [4.6447937e-07, 2.0228642e-08, 3.1893014e-05, 1.3218335e-07,\n",
              "        9.9132991e-01, 1.5570490e-07, 1.0622097e-06, 3.2979235e-05,\n",
              "        3.1157600e-05, 8.5721454e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4ikO1fTF0jV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}